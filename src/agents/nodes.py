"""
LangGraph Agent èŠ‚ç‚¹å®šä¹‰
ä½¿ç”¨ LangGraph æ ‡å‡†æ¨¡å¼ï¼šbind_tools + ToolNode + add_messages
"""
import json
import time
import re
from typing import Optional, Callable, List, Dict, Any
from langchain_core.messages import AIMessage, HumanMessage, BaseMessage, ToolMessage
from langgraph.prebuilt import ToolNode
from ..core.state import AgentState
from ..core.llm import get_llm
from ..tools.traffic_tools import traffic_prediction, anomaly_detection, causal_analysis, travel_recommendation
from ..tools.mcp_client import get_mcp_tools_sync

# å…¨å±€çŠ¶æ€å›è°ƒå‡½æ•°
_status_callback: Optional[Callable[[str, str, str], None]] = None


def set_status_callback(callback: Optional[Callable[[str, str, str], None]]):
    """è®¾ç½®çŠ¶æ€å›è°ƒå‡½æ•°"""
    global _status_callback
    _status_callback = callback


def _notify_status(phase: str, text: str, detail: str = ""):
    """é€šçŸ¥çŠ¶æ€å˜åŒ–"""
    if _status_callback:
        try:
            _status_callback(phase, text, detail)
        except Exception as e:
            print(f"Status callback error: {e}")


# --- Debug è¾…åŠ©å‡½æ•° ---
def _add_debug_log(state: AgentState, log_type: str, content: dict) -> None:
    if "debug_logs" not in state or state["debug_logs"] is None:
        state["debug_logs"] = []
    state["debug_logs"].append({
        "timestamp": time.strftime("%H:%M:%S"),
        "type": log_type,
        "content": content
    })


# --- å·¥å…·è·å– ---
# æœ¬åœ°å·¥å…·åˆ—è¡¨
LOCAL_TOOLS = [
    traffic_prediction,
    anomaly_detection,
    causal_analysis,
    travel_recommendation,
]


def get_all_tools() -> List:
    """è·å–æ‰€æœ‰å·¥å…·ï¼ˆæœ¬åœ°å·¥å…· + MCP å·¥å…·ï¼‰"""
    tools = LOCAL_TOOLS.copy()
    try:
        mcp_tools = get_mcp_tools_sync()
        if mcp_tools:
            tools.extend(mcp_tools)
            print(f"âœ… Loaded {len(mcp_tools)} MCP tools")
    except Exception as e:
        print(f"âš ï¸ Failed to load MCP tools: {e}")
    return tools


# å…¨å±€å·¥å…·åˆ—è¡¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_all_tools: List | None = None


def _get_tools():
    """è·å–å·¥å…·åˆ—è¡¨ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    global _all_tools
    if _all_tools is None:
        _all_tools = get_all_tools()
    return _all_tools


# --- æ¶ˆæ¯é¢„å¤„ç† ---
def _normalize_messages(messages: List[BaseMessage]) -> List[BaseMessage]:
    """
    è§„èŒƒåŒ–æ¶ˆæ¯åˆ—è¡¨ï¼š
    1. å°† ToolMessage çš„ list content è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    """
    normalized = []
    for msg in messages:
        # å¤„ç† ToolMessage çš„ content ä¸º list çš„æƒ…å†µ (MCP å·¥å…·è¿”å› TextContent åˆ—è¡¨)
        if isinstance(msg, ToolMessage) and isinstance(msg.content, list):
            text_parts = []
            for item in msg.content:
                if isinstance(item, dict) and 'text' in item:
                    text_parts.append(item['text'])
                elif isinstance(item, str):
                    text_parts.append(item)
                else:
                    text_parts.append(str(item))
            # åˆ›å»ºæ–°çš„ ToolMessage
            msg = ToolMessage(
                content='\n'.join(text_parts),
                tool_call_id=msg.tool_call_id,
                name=getattr(msg, 'name', 'unknown')
            )
        normalized.append(msg)
    return normalized


# --- æ ¸å¿ƒèŠ‚ç‚¹é€»è¾‘ ---

def perception_node(state: AgentState) -> Dict[str, Any]:
    """æ„ŸçŸ¥èŠ‚ç‚¹ï¼šæå–æ„å›¾ï¼Œè¿”å›æ›´æ–°"""
    print("ğŸ” [Perception] Extracting intent & locations...")
    _notify_status("perception", "ğŸ” æ­£åœ¨æ„ŸçŸ¥ç”¨æˆ·æ„å›¾...", "åˆ†ææ‚¨çš„é—®é¢˜")
    
    # æ·»åŠ è°ƒè¯•æ—¥å¿—
    _add_debug_log(state, "perception", {"action": "å¼€å§‹æå–ç”¨æˆ·æ„å›¾å’Œåœ°ç‚¹ä¿¡æ¯"})
    
    user_request = state["user_request"]
    llm = get_llm()
    
    # æå–èµ·ç‚¹ç»ˆç‚¹
    prompt = f"""Extract origin and destination from: "{user_request}".
Return JSON ONLY: {{"origin": "...", "destination": "..."}}. 
If unknown, use empty string."""
    
    origin = ""
    destination = ""
    
    try:
        response = llm.invoke(prompt)
        content = response.content.strip().replace("```json", "").replace("```", "")
        match = re.search(r"\{.*?\}", content, re.DOTALL)
        if match:
            data = json.loads(match.group(0))
            origin = data.get("origin", "")
            destination = data.get("destination", "")
            print(f"   ğŸ“ Extracted: {origin} -> {destination}")
            _add_debug_log(state, "perception", {
                "action": "åœ°ç‚¹æå–å®Œæˆ",
                "origin": origin,
                "destination": destination
            })
    except Exception as e:
        print(f"   âš ï¸ Perception failed: {e}")
        _add_debug_log(state, "perception", {"action": "åœ°ç‚¹æå–å¤±è´¥", "error": str(e)})

    # è¿”å›çŠ¶æ€æ›´æ–°ï¼ˆmessages ä¼šè‡ªåŠ¨è¿½åŠ ï¼‰
    return {
        "origin": origin,
        "destination": destination,
        "current_step": "perception",
        "tool_outputs": [],
        "retry_count": 0,
        "messages": [HumanMessage(content=user_request)],
        "debug_logs": state.get("debug_logs", [])
    }


def call_model(state: AgentState) -> Dict[str, Any]:
    """è°ƒç”¨æ¨¡å‹èŠ‚ç‚¹ï¼šä½¿ç”¨ bind_tools è®© LLM å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·"""
    retry_count = state.get("retry_count", 0)
    print(f"ğŸ“‹ [Call Model] Reasoning... (attempt {retry_count + 1})")
    _notify_status("planning", "ğŸ“‹ æ­£åœ¨è§„åˆ’æ–¹æ¡ˆ...", f"æ¨¡å‹æ€è€ƒä¸­ (å°è¯• {retry_count + 1})")
    
    _add_debug_log(state, "call_model", {"action": f"å¼€å§‹æ¨ç† (ç¬¬ {retry_count + 1} æ¬¡)"})
    
    # è·å–å·¥å…·å¹¶ç»‘å®šåˆ° LLM
    tools = _get_tools()
    llm = get_llm()
    llm_with_tools = llm.bind_tools(tools)
    
    # æ„å»ºç³»ç»Ÿæç¤º
    origin = state.get("origin", "")
    destination = state.get("destination", "")
    
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªäº¤é€šæ™ºèƒ½ä½“åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œä½¿ç”¨æä¾›çš„å·¥å…·æ¥è·å–ä¿¡æ¯å¹¶å›ç­”ã€‚

**é‡è¦è§„åˆ™**ï¼š
1. æ¶‰åŠåœ°ç‚¹æŸ¥è¯¢ã€è·¯çº¿è§„åˆ’æ—¶ï¼Œå¿…é¡»ä½¿ç”¨å·¥å…·è·å–å®æ—¶æ•°æ®
2. é«˜å¾·åœ°å›¾ API ä»…æ”¯æŒç»çº¬åº¦ï¼Œå¿…é¡»å…ˆè°ƒç”¨ maps_geo è·å–åæ ‡
3. è·å–åæ ‡åï¼Œå†è°ƒç”¨ maps_direction_* è·å–è·¯çº¿
4. ä¸è¦ç¼–é€ æ•°æ®ï¼Œå¿…é¡»åŸºäºå·¥å…·è¿”å›çš„çœŸå®æ•°æ®å›ç­”"""

    if origin and destination:
        system_prompt += f"\n\n[å·²çŸ¥ä¿¡æ¯] èµ·ç‚¹: {origin}, ç»ˆç‚¹: {destination}"
    
    # è·å–æ¶ˆæ¯åˆ—è¡¨å¹¶è§„èŒƒåŒ–
    messages = _normalize_messages(list(state.get("messages", [])))
    
    # åœ¨ç¬¬ä¸€æ¡æ¶ˆæ¯å‰æ·»åŠ ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
    if messages and isinstance(messages[0], HumanMessage):
        if "äº¤é€šæ™ºèƒ½ä½“" not in messages[0].content:
            messages[0] = HumanMessage(content=f"{system_prompt}\n\nç”¨æˆ·é—®é¢˜ï¼š{messages[0].content}")
    
    try:
        # è°ƒç”¨å¸¦å·¥å…·çš„ LLM
        response = llm_with_tools.invoke(messages)
        
        # è®°å½•å“åº”
        if response.tool_calls:
            tool_names = [tc["name"] for tc in response.tool_calls]
            print(f"   ğŸ› ï¸ Model requested tools: {tool_names}")
            _add_debug_log(state, "call_model", {
                "action": "LLM è¯·æ±‚å·¥å…·è°ƒç”¨",
                "tools": tool_names
            })
            _notify_status("execution", "âš¡ æ­£åœ¨æ‰§è¡Œå·¥å…·...", f"è°ƒç”¨: {', '.join(tool_names)}")
        else:
            content_preview = response.content[:100] if response.content else "(empty)"
            print(f"   ğŸ’¬ Model response: {content_preview}...")
            _add_debug_log(state, "call_model", {
                "action": "LLM ç›´æ¥å“åº”",
                "content": content_preview
            })
        
        return {
            "messages": [response],
            "retry_count": retry_count + 1,
            "current_step": "call_model",
            "debug_logs": state.get("debug_logs", [])
        }
        
    except Exception as e:
        print(f"   âŒ LLM Error: {e}")
        _add_debug_log(state, "call_model", {"action": "LLM é”™è¯¯", "error": str(e)})
        return {
            "messages": [AIMessage(content=f"æŠ±æ­‰ï¼Œå¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")],
            "retry_count": retry_count + 1,
            "current_step": "call_model",
            "debug_logs": state.get("debug_logs", [])
        }


def output_node(state: AgentState) -> Dict[str, Any]:
    """è¾“å‡ºèŠ‚ç‚¹ï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    print("âœ… [Output] Generating report...")
    _notify_status("output", "ğŸ“ æ­£åœ¨ç”ŸæˆæŠ¥å‘Š...", "æ•´åˆç»“æœ")
    
    _add_debug_log(state, "final_output", {"action": "å¼€å§‹ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"})
    
    messages = state.get("messages", [])
    
    # è·å–æœ€åä¸€æ¡ AI æ¶ˆæ¯ä½œä¸ºå“åº”
    final_response = ""
    for msg in reversed(messages):
        if isinstance(msg, AIMessage) and msg.content and not msg.tool_calls:
            final_response = msg.content
            break
    
    # å¦‚æœæ²¡æœ‰ç›´æ¥å“åº”ï¼ŒåŸºäºå·¥å…·è¾“å‡ºç”ŸæˆæŠ¥å‘Š
    if not final_response or final_response.startswith("æŠ±æ­‰"):
        # ä»æ¶ˆæ¯ä¸­æ”¶é›†å·¥å…·è¾“å‡º
        tool_outputs = []
        for msg in messages:
            if isinstance(msg, ToolMessage):
                content = msg.content
                if isinstance(content, list):
                    # å¤„ç† list content
                    for item in content:
                        if isinstance(item, dict) and 'text' in item:
                            tool_outputs.append({"tool": getattr(msg, 'name', 'unknown'), "output": item['text']})
                else:
                    tool_outputs.append({"tool": getattr(msg, 'name', 'unknown'), "output": content})
        
        if tool_outputs:
            llm = get_llm()
            context = json.dumps(tool_outputs, ensure_ascii=False, indent=2)
            prompt = f"""æ ¹æ®ä»¥ä¸‹å·¥å…·è¿”å›çš„æ•°æ®ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·ç›´æ¥ç»™å‡ºæœ‰ç”¨çš„ä¿¡æ¯ï¼Œä¸è¦æåŠ"å·¥å…·"æˆ–"API"ã€‚

å·¥å…·æ•°æ®ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{state['user_request']}

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š"""
            
            response = llm.invoke([HumanMessage(content=prompt)])
            final_response = response.content
    
    _add_debug_log(state, "final_output", {
        "action": "æŠ¥å‘Šç”Ÿæˆå®Œæˆ",
        "length": len(final_response)
    })
    
    return {
        "recommendation": final_response,
        "current_step": "output",
        "debug_logs": state.get("debug_logs", [])
    }


def create_tool_node():
    """åˆ›å»º ToolNode å®ä¾‹"""
    tools = _get_tools()
    return ToolNode(tools)


def should_continue(state: AgentState) -> str:
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­è°ƒç”¨å·¥å…·
    
    Returns:
        "tools": å¦‚æœéœ€è¦æ‰§è¡Œå·¥å…·
        "output": å¦‚æœå¯ä»¥ç”Ÿæˆæœ€ç»ˆè¾“å‡º
    """
    messages = state.get("messages", [])
    retry_count = state.get("retry_count", 0)
    
    # æ£€æŸ¥é‡è¯•æ¬¡æ•°
    if retry_count >= 5:
        print("   âš ï¸ Max retries reached")
        return "output"
    
    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯
    if messages:
        last_message = messages[-1]
        
        # å¦‚æœæ˜¯ AIMessage ä¸”æœ‰ tool_callsï¼Œæ‰§è¡Œå·¥å…·
        if isinstance(last_message, AIMessage) and last_message.tool_calls:
            return "tools"
    
    return "output"