import json
import time
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage, ToolMessage
from ..core.state import AgentState
from ..core.llm import get_llm
from ..tools.traffic_tools import traffic_prediction, anomaly_detection, causal_analysis, travel_recommendation
from ..tools.maps import route_planning

llm = get_llm()
tools = [traffic_prediction, anomaly_detection, causal_analysis, travel_recommendation, route_planning]
llm_with_tools = llm.bind_tools(tools)

# æœ€å¤§é‡è¯•æ¬¡æ•°
MAX_RETRY_COUNT = 2

def _add_debug_log(state: AgentState, log_type: str, content: dict) -> None:
    """æ·»åŠ è°ƒè¯•æ—¥å¿—åˆ° state"""
    if "debug_logs" not in state or state["debug_logs"] is None:
        state["debug_logs"] = []
    state["debug_logs"].append({
        "timestamp": time.strftime("%H:%M:%S"),
        "type": log_type,
        "content": content
    })

def perception_node(state: AgentState) -> AgentState:
    """æ„ŸçŸ¥èŠ‚ç‚¹ï¼šè·å–äº¤é€šæ€åŠ¿"""
    print("ğŸ” [Perception] Detecting traffic status...")
    
    # åˆå§‹åŒ– debug_logs
    if "debug_logs" not in state or state["debug_logs"] is None:
        state["debug_logs"] = []
    
    # è¿™é‡Œæ¨¡æ‹Ÿä¸€æ¬¡ç®€å•çš„æ„ŸçŸ¥ï¼Œå¦‚æœç”¨æˆ·æ²¡æä¾›èµ·ç‚¹ç»ˆç‚¹ï¼Œå…ˆå°è¯•æå–
    if not state.get("origin") or not state.get("destination"):
        # ç®€å•ä»æ¶ˆæ¯ä¸­æå–ï¼ˆå®é™…å¯ä»¥ç”¨LLMè¾…åŠ©ï¼‰
        msg = state["user_request"]
        # ç®€å•é€»è¾‘æ¼”ç¤º
        if "åˆ°" in msg:
            parts = msg.split("åˆ°")
            state["origin"] = parts[0].replace("ä»", "").strip()
            state["destination"] = parts[1].split("ï¼Œ")[0].split(" ")[0].strip()
    
    _add_debug_log(state, "perception", {
        "origin": state.get("origin", "æœªçŸ¥"),
        "destination": state.get("destination", "æœªçŸ¥")
    })
    
    # è°ƒç”¨å¼‚å¸¸æ£€æµ‹
    res = anomaly_detection.invoke({"location": state.get("origin", "æœªçŸ¥åŒºåŸŸ")})
    state["traffic_status"] = str(res)
    state["messages"].append(AIMessage(content=f"å·²å®Œæˆåˆæ­¥äº¤é€šæ€åŠ¿æ„ŸçŸ¥ï¼š{state['traffic_status']}"))
    state["current_step"] = "perception"
    
    _add_debug_log(state, "perception_result", {
        "traffic_status": state["traffic_status"]
    })
    
    return state

def planning_node(state: AgentState) -> AgentState:
    """è§„åˆ’èŠ‚ç‚¹ï¼šè°ƒç”¨å·¥å…·å¹¶ç”Ÿæˆå€™é€‰æ–¹æ¡ˆ"""
    retry_count = state.get("retry_count", 0)
    print(f"ğŸ“‹ [Planning] Generating plans... (attempt {retry_count + 1}/{MAX_RETRY_COUNT + 1})")
    
    # åŒ…å«æ„ŸçŸ¥ä¿¡æ¯å’Œå†å²æ¶ˆæ¯
    messages = [
        SystemMessage(content=f"""ä½ æ˜¯ä¸€ä¸ªäº¤é€šä¸“å®¶ã€‚å½“å‰äº¤é€šæ€åŠ¿ä¸ºï¼š{state.get('traffic_status', 'æœªçŸ¥')}ã€‚
è¯·æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚ï¼Œè°ƒç”¨åˆé€‚çš„å·¥å…·ï¼ˆé¢„æµ‹ã€å¼‚å¸¸ã€å› æœã€è·¯å¾„è§„åˆ’ã€æ¨èï¼‰æ¥ç”Ÿæˆæœ€ä½³å»ºè®®ã€‚
é‡è¦æç¤ºï¼šè¯·å°½é‡ä¸€æ¬¡æ€§è°ƒç”¨æ‰€æœ‰éœ€è¦çš„å·¥å…·ï¼Œé¿å…å¤šæ¬¡åå¤è°ƒç”¨ã€‚"""),
        HumanMessage(content=state["user_request"])
    ]
    # åŒ…å«ä¹‹å‰çš„å·¥å…·è¾“å‡ºå’Œæ¨¡å‹æ€è€ƒï¼ˆå¦‚æœæœ‰ï¼‰
    if state.get("messages"):
        messages.extend(state["messages"])
    
    # è°ƒç”¨ LLM
    start_time = time.time()
    response = llm_with_tools.invoke(messages)
    elapsed_time = time.time() - start_time
    
    # è®°å½• LLM å®Œæ•´è¾“å‡ºåˆ° debug_logs
    _add_debug_log(state, "llm_response", {
        "content": response.content if response.content else "(æ— æ–‡æœ¬å†…å®¹)",
        "tool_calls": [
            {"name": tc["name"], "args": tc["args"]} 
            for tc in (response.tool_calls or [])
        ],
        "elapsed_time": f"{elapsed_time:.2f}s"
    })
    
    print(f"   ğŸ“ LLM Response: {response.content[:100] if response.content else '(å·¥å…·è°ƒç”¨)'}...")
    
    state["messages"].append(response)
    
    # æ‰§è¡Œå·¥å…·è°ƒç”¨
    outputs = state.get("tool_outputs", [])
    if response.tool_calls:
        for tool_call in response.tool_calls:
            tool_name = tool_call["name"]
            tool_args = tool_call["args"]
            print(f"   ğŸ› ï¸ Calling tool: {tool_name} with {tool_args}")
            
            try:
                if tool_name == "traffic_prediction":
                    tool_output = traffic_prediction.invoke(tool_args)
                elif tool_name == "anomaly_detection":
                    tool_output = anomaly_detection.invoke(tool_args)
                elif tool_name == "causal_analysis":
                    tool_output = causal_analysis.invoke(tool_args)
                elif tool_name == "travel_recommendation":
                    tool_output = travel_recommendation.invoke(tool_args)
                elif tool_name == "route_planning":
                    tool_output = route_planning.invoke(tool_args)
                else:
                    tool_output = {"error": f"Unknown tool: {tool_name}"}
                
                outputs.append({"tool": tool_name, "output": tool_output})
                
                # è®°å½•å·¥å…·è°ƒç”¨ç»“æœ
                _add_debug_log(state, "tool_execution", {
                    "tool": tool_name,
                    "args": tool_args,
                    "output": tool_output
                })
                
                # å°†å·¥å…·ç»“æœä½œä¸º ToolMessage å›ä¼ ç»™å¯¹è¯æµï¼Œé˜²æ­¢ LLM é‡å¤è°ƒç”¨
                state["messages"].append(ToolMessage(
                    content=str(tool_output),
                    tool_call_id=tool_call["id"]
                ))
            except Exception as e:
                print(f"   âŒ Tool execution failed: {e}")
                _add_debug_log(state, "tool_error", {
                    "tool": tool_name,
                    "error": str(e)
                })
    else:
        print("   â„¹ï¸ No tool calls in response")
        _add_debug_log(state, "no_tool_calls", {
            "note": "LLM æœªè¿”å›å·¥å…·è°ƒç”¨ï¼Œå¯èƒ½å·²æœ‰è¶³å¤Ÿä¿¡æ¯"
        })
    
    state["tool_outputs"] = outputs
    state["current_step"] = "planning"
    return state

def reflection_node(state: AgentState) -> AgentState:
    """åæ€èŠ‚ç‚¹ï¼šè¯„ä¼°æ–¹æ¡ˆ"""
    print("ğŸ¤” [Reflection] Evaluating plans...")
    
    # å¢åŠ é‡è¯•è®¡æ•°
    retry_count = state.get("retry_count", 0) + 1
    state["retry_count"] = retry_count
    
    # è¯„ä¼°é€»è¾‘ä¼˜åŒ–ï¼š
    # 1. å¦‚æœæœ‰å·¥å…·äº§å‡ºï¼Œç›´æ¥é€šè¿‡
    # 2. å¦‚æœé‡è¯•æ¬¡æ•°è¾¾åˆ°ä¸Šé™ï¼Œä¹Ÿé€šè¿‡ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
    # 3. å¦‚æœ LLM æœ‰å“åº”å†…å®¹ä½†æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¯´æ˜å·²æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œä¹Ÿé€šè¿‡
    
    has_tool_outputs = bool(state.get("tool_outputs"))
    max_retries_reached = retry_count >= MAX_RETRY_COUNT
    
    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦æ˜¯æœ‰å†…å®¹çš„ AI å“åº”
    last_ai_message = None
    for msg in reversed(state.get("messages", [])):
        if isinstance(msg, AIMessage) and not hasattr(msg, 'tool_calls'):
            last_ai_message = msg
            break
        elif isinstance(msg, AIMessage) and msg.content:
            last_ai_message = msg
            break
    
    has_meaningful_response = last_ai_message and last_ai_message.content
    
    if has_tool_outputs or max_retries_reached or has_meaningful_response:
        state["reflection_score"] = 0.9
        reason = []
        if has_tool_outputs:
            reason.append("æœ‰å·¥å…·è¾“å‡º")
        if has_meaningful_response:
            reason.append("æœ‰LLMå“åº”")
        if max_retries_reached:
            reason.append("è¾¾åˆ°é‡è¯•ä¸Šé™")
        print(f"   âœ… Passed: {', '.join(reason)}")
    else:
        state["reflection_score"] = 0.5
        print(f"   âš ï¸ Retry needed (attempt {retry_count}/{MAX_RETRY_COUNT})")
    
    _add_debug_log(state, "reflection", {
        "retry_count": retry_count,
        "reflection_score": state["reflection_score"],
        "has_tool_outputs": has_tool_outputs,
        "max_retries_reached": max_retries_reached
    })
    
    state["current_step"] = "reflection"
    return state

def output_node(state: AgentState) -> AgentState:
    """è¾“å‡ºèŠ‚ç‚¹ï¼šç”ŸæˆæŠ¥å‘Š"""
    print("âœ… [Output] Generating final report...")
    
    context = f"ç”¨æˆ·éœ€æ±‚: {state['user_request']}\næ€åŠ¿: {state['traffic_status']}\nå·¥å…·ç»“æœ: {json.dumps(state['tool_outputs'], ensure_ascii=False)}"
    
    prompt = f"è¯·æ ¹æ®ä»¥ä¸‹èƒŒæ™¯ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·ç”Ÿæˆä¸€ä¸ªä¸“ä¸šã€å‹å¥½ä¸”è¯¦å°½çš„äº¤é€šè¯±å¯¼æŠ¥å‘Šã€‚åŒ…å«é¢„æµ‹ã€å¼‚å¸¸ã€å› æœå’Œå…·ä½“å»ºè®®ã€‚\n\n{context}"
    
    start_time = time.time()
    response = llm.invoke([HumanMessage(content=prompt)])
    elapsed_time = time.time() - start_time
    
    state["recommendation"] = response.content
    state["messages"].append(AIMessage(content="æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ã€‚"))
    state["current_step"] = "output"
    
    _add_debug_log(state, "final_output", {
        "report_length": len(response.content),
        "elapsed_time": f"{elapsed_time:.2f}s"
    })
    
    return state
